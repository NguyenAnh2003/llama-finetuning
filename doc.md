# Introduction
1. LLM ranging from 7B to 65B parameters.
2. Trained on unlabeled datasets
3. Auto regressive model

# Target
Elementary Maths Solving

# Keywords
PEFT: traditional fine-tuning requires updating all of the model's params, which is expensive
    This work only updating a small subset of the model's parameters
Matrix decomposition.
Intrinsic dimension.

# Libs
LoRA: https://www.youtube.com/watch?v=t509sv5MT0w
QLoRA

# External guides
https://www.youtube.com/watch?v=Us5ZFp16PaU
https://www.youtube.com/watch?v=MDA3LUKNl1E

# References
1. https://github.com/VietnamAIHub/Vietnamese_LLMs
2. https://www.datacamp.com/tutorial/fine-tuning-llama-2
3. https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language
4. https://viblo.asia/p/fine-tuning-mot-cach-hieu-qua-va-than-thien-voi-phan-cung-adapters-va-lora-5pPLkj3eJRZ
5. https://www.youtube.com/watch?v=YT3VSlDjrVU
6. https://www.youtube.com/watch?v=J_3hDqSvpmg
7. Quantization: https://www.youtube.com/watch?v=v1oHf1KV6kM
